{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4886ac-f468-4644-9724-4f1725b2ffaa",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fe8ba-0ce0-4c78-9dbe-6f591fccadde",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to scale and transform the values of numerical features within a specific range. The purpose of Min-Max scaling is to bring all the feature values into a common scale, usually between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "\\[X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "Here:\n",
    "- \\(X_{\\text{normalized}}\\) is the normalized value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "The normalized value (\\(X_{\\text{normalized}}\\)) will be between 0 and 1, inclusive.\n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Let's consider a dataset with a feature representing the salary of individuals. The salary values range from $40,000 to $100,000. We want to apply Min-Max scaling to normalize these values between 0 and 1.\n",
    "\n",
    "1. Original dataset:\n",
    "\n",
    "   | Salary   |\n",
    "   |----------|\n",
    "   | $40,000  |\n",
    "   | $60,000  |\n",
    "   | $80,000  |\n",
    "   | $100,000 |\n",
    "\n",
    "2. Apply Min-Max scaling:\n",
    "\n",
    "   \\[X_{\\text{normalized}} = \\frac{X - 40,000}{100,000 - 40,000}\\]\n",
    "\n",
    "   | Salary   | Normalized Salary |\n",
    "   |----------|-------------------|\n",
    "   | $40,000  | 0.0               |\n",
    "   | $60,000  | 0.333             |\n",
    "   | $80,000  | 0.667             |\n",
    "   | $100,000 | 1.0               |\n",
    "\n",
    "After Min-Max scaling, the salary values are transformed into a range between 0 and 1, making them comparable and suitable for machine learning algorithms that are sensitive to the scale of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972a22d-1ed9-4bfe-8bbf-377bc17b3b58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a72af8d5-f8e5-490b-842b-fdd19a132416",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae89669-ac9b-4d0f-b116-d72741816db3",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or unit normalization, is a feature scaling method that scales each data point to have a magnitude of 1 while preserving the direction of the original vector. In the context of feature scaling, it is often applied to normalize the feature vectors rather than individual features. This normalization is particularly useful when the direction of the feature vector is important, such as in cases where the magnitude of the vector is not significant but the direction is relevant.\n",
    "\n",
    "The formula for unit vector scaling is as follows:\n",
    "\n",
    "\\[X_{\\text{unit}} = \\frac{X}{\\|X\\|}\\]\n",
    "\n",
    "Here:\n",
    "- \\(X_{\\text{unit}}\\) is the unit vector of the original vector \\(X\\).\n",
    "- \\(X\\) is the original feature vector.\n",
    "- \\(\\|X\\|\\) is the Euclidean norm (magnitude) of the vector \\(X\\).\n",
    "\n",
    "The resulting \\(X_{\\text{unit}}\\) will have a magnitude of 1.\n",
    "\n",
    "Now, let's compare Unit Vector scaling with Min-Max scaling using an example:\n",
    "\n",
    "Consider a dataset with two features: Salary and Years of Experience. We want to scale the feature vectors using both Min-Max scaling and Unit Vector scaling.\n",
    "\n",
    "1. Original dataset:\n",
    "\n",
    "   | Salary   | Years of Experience |\n",
    "   |----------|---------------------|\n",
    "   | $40,000  | 2                   |\n",
    "   | $60,000  | 4                   |\n",
    "   | $80,000  | 6                   |\n",
    "   | $100,000 | 8                   |\n",
    "\n",
    "2. Apply Min-Max scaling:\n",
    "\n",
    "   Min-Max scaling for Salary:\n",
    "   \\[X_{\\text{normalized}} = \\frac{X - 40,000}{100,000 - 40,000}\\]\n",
    "\n",
    "   Min-Max scaling for Years of Experience:\n",
    "   \\[X_{\\text{normalized}} = \\frac{X - 2}{8 - 2}\\]\n",
    "\n",
    "   | Salary   | Years of Experience | Min-Max Scaled Salary | Min-Max Scaled Exp. |\n",
    "   |----------|---------------------|-----------------------|---------------------|\n",
    "   | $40,000  | 2                   | 0.0                   | 0.0                 |\n",
    "   | $60,000  | 4                   | 0.333                 | 0.333               |\n",
    "   | $80,000  | 6                   | 0.667                 | 0.667               |\n",
    "   | $100,000 | 8                   | 1.0                   | 1.0                 |\n",
    "\n",
    "3. Apply Unit Vector scaling:\n",
    "\n",
    "   \\[X_{\\text{unit}} = \\frac{X}{\\|X\\|}\\]\n",
    "\n",
    "   | Salary   | Years of Experience | Unit Scaled Salary | Unit Scaled Exp. |\n",
    "   |----------|---------------------|--------------------|------------------|\n",
    "   | $40,000  | 2                   | 0.447              | 0.089            |\n",
    "   | $60,000  | 4                   | 0.447              | 0.178            |\n",
    "   | $80,000  | 6                   | 0.447              | 0.267            |\n",
    "   | $100,000 | 8                   | 0.447              | 0.356            |\n",
    "\n",
    "In Min-Max scaling, each feature is scaled independently to the range [0, 1]. In Unit Vector scaling, the entire feature vector is scaled such that its magnitude is 1, preserving the direction of the original vector. This is particularly useful when the direction or relative importance of the features is more critical than their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d7237-52af-4ea2-8ec2-029761ca6884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc0f0ca3-86c8-402d-83f2-1d0c2a4b4823",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5a1e7-b531-4d61-b02f-eed18680a2c9",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and statistics. Its primary goal is to transform the original features of a dataset into a new set of uncorrelated features, called principal components, that capture the maximum variance in the data. PCA is especially useful when dealing with high-dimensional datasets, as it helps to reduce the number of features while retaining most of the essential information.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. **Standardize the data:** If the features have different scales, it's essential to standardize them to have zero mean and unit variance.\n",
    "\n",
    "2. **Compute the covariance matrix:** Calculate the covariance matrix of the standardized data, which represents the relationships between different features.\n",
    "\n",
    "3. **Compute eigenvectors and eigenvalues:** The eigenvectors and eigenvalues of the covariance matrix are calculated. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of variance in those directions.\n",
    "\n",
    "4. **Sort eigenvectors by eigenvalues:** Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvectors with the highest eigenvalues are the principal components that capture the most variance.\n",
    "\n",
    "5. **Choose the number of principal components:** Decide on the number of principal components to retain. This is often determined by selecting the top k eigenvectors that explain a significant portion of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "6. **Project the data onto the new feature space:** Construct a new feature space using the selected principal components and project the original data onto this reduced-dimensional space.\n",
    "\n",
    "Here's an example to illustrate PCA:\n",
    "\n",
    "Consider a dataset with two features, \"Income\" and \"Expenditure,\" measured in thousands of dollars. We want to apply PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "1. Original dataset:\n",
    "\n",
    "   | Income | Expenditure |\n",
    "   |--------|-------------|\n",
    "   | 50     | 30          |\n",
    "   | 60     | 50          |\n",
    "   | 45     | 25          |\n",
    "   | 75     | 80          |\n",
    "\n",
    "2. Standardize the data:\n",
    "\n",
    "   Subtract the mean and divide by the standard deviation for each feature.\n",
    "\n",
    "3. Compute the covariance matrix:\n",
    "\n",
    "   \\[\\text{Covariance Matrix} = \\begin{bmatrix} \\text{Var}(\\text{Income}) & \\text{Cov}(\\text{Income, Expenditure}) \\\\ \\text{Cov}(\\text{Income, Expenditure}) & \\text{Var}(\\text{Expenditure}) \\end{bmatrix}\\]\n",
    "\n",
    "4. Compute eigenvectors and eigenvalues:\n",
    "\n",
    "   Solve the characteristic equation \\(\\text{det}(\\text{Covariance Matrix} - \\lambda \\cdot \\text{Identity Matrix}) = 0\\) to find eigenvalues (\\(\\lambda\\)) and corresponding eigenvectors.\n",
    "\n",
    "5. Sort eigenvectors by eigenvalues:\n",
    "\n",
    "   Select the top eigenvectors based on their eigenvalues.\n",
    "\n",
    "6. Project the data onto the new feature space:\n",
    "\n",
    "   Multiply the original data by the selected eigenvectors to obtain the reduced-dimensional representation.\n",
    "\n",
    "The resulting dataset will have fewer features (principal components) that capture the most significant variance in the original data. PCA is a powerful tool for dimensionality reduction, noise reduction, and visualization of high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad337472-7872-4974-8863-a3bd581175a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0113c6d6-d006-4638-a5d2-7ff1601ddeb8",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c83391c-827c-4518-9659-74683f0b4a63",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that can be used for feature extraction. In the context of PCA, feature extraction refers to the process of transforming the original features of a dataset into a new set of features, called principal components, while retaining most of the important information. These principal components are linear combinations of the original features and are chosen to capture the maximum variance in the data.\n",
    "\n",
    "The relationship between PCA and feature extraction can be understood as follows:\n",
    "\n",
    "1. **Transformation of Features:**\n",
    "   - In PCA, the original features of the dataset are linearly transformed into a new set of features (principal components).\n",
    "   - These principal components are orthogonal (uncorrelated) and are sorted by the amount of variance they capture.\n",
    "\n",
    "2. **Reduction in Dimensionality:**\n",
    "   - Feature extraction using PCA results in a reduced-dimensional representation of the data.\n",
    "   - The number of principal components retained determines the dimensionality of the reduced space.\n",
    "\n",
    "3. **Retained Information:**\n",
    "   - PCA aims to retain as much of the original variance in the data as possible.\n",
    "   - The first few principal components typically capture the majority of the variance, allowing for dimensionality reduction without significant loss of information.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Consider a dataset with three features: \"Height,\" \"Weight,\" and \"Age\" of individuals. We want to use PCA to extract new features that capture the most significant information in the data.\n",
    "\n",
    "1. Original dataset:\n",
    "\n",
    "   | Height | Weight | Age |\n",
    "   |--------|--------|-----|\n",
    "   | 170    | 65     | 30  |\n",
    "   | 155    | 50     | 25  |\n",
    "   | 180    | 75     | 35  |\n",
    "   | 165    | 60     | 28  |\n",
    "\n",
    "2. Apply PCA:\n",
    "\n",
    "   - Standardize the data (subtract mean and divide by standard deviation).\n",
    "   - Compute the covariance matrix.\n",
    "   - Calculate eigenvectors and eigenvalues.\n",
    "   - Sort eigenvectors by eigenvalues and select the top k eigenvectors.\n",
    "\n",
    "   Let's say we choose to retain two principal components.\n",
    "\n",
    "   | Principal Component 1 | Principal Component 2 |\n",
    "   |-----------------------|-----------------------|\n",
    "   | 0.67                  | -0.35                 |\n",
    "   | -0.71                 | 0.38                  |\n",
    "   | 0.83                  | 0.43                  |\n",
    "   | -0.78                 | -0.40                 |\n",
    "\n",
    "3. Project the data onto the new feature space:\n",
    "\n",
    "   Multiply the original data by the selected eigenvectors to obtain the reduced-dimensional representation:\n",
    "\n",
    "   | PC1 Projection | PC2 Projection |\n",
    "   |-----------------|-----------------|\n",
    "   | 0.49            | -0.29           |\n",
    "   | -0.47           | 0.26            |\n",
    "   | 0.68            | 0.37            |\n",
    "   | -0.69           | -0.33           |\n",
    "\n",
    "The resulting data with PC1 and PC2 as features represents a compressed version of the original data while capturing the maximum variance. These new features can be used in place of the original features for various machine learning tasks. The reduction in dimensionality simplifies the data while retaining essential patterns, making it useful for more efficient and effective analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d16bb-43a5-443b-a199-2929faa95eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34353df6-d985-496d-9aa6-5907aa1e9985",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63ccb7-622a-48aa-b6c0-106b139dd90e",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be employed as a preprocessing step to ensure that the numerical features, such as price, rating, and delivery time, are on a consistent scale. This is important because recommendation algorithms often rely on the similarity or distance between items, and having features on a common scale can help prevent certain features from disproportionately influencing the recommendation process.\n",
    "\n",
    "Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. **Understand the Data:**\n",
    "   - Examine the dataset to identify numerical features that need scaling. In your case, this might include features like price, rating, and delivery time.\n",
    "\n",
    "2. **Compute Min and Max Values:**\n",
    "   - Calculate the minimum (\\(X_{\\text{min}}\\)) and maximum (\\(X_{\\text{max}}\\)) values for each numerical feature in the dataset.\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "   - Use the Min-Max scaling formula for each feature:\n",
    "     \\[X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "   - This formula scales each feature's values to the range [0, 1].\n",
    "\n",
    "4. **Apply Scaling to the Dataset:**\n",
    "   - Replace the original values of each numerical feature with their corresponding scaled values.\n",
    "\n",
    "Let's illustrate this with an example:\n",
    "\n",
    "Consider a simplified subset of the dataset with features like price, rating, and delivery time for different food items:\n",
    "\n",
    "```plaintext\n",
    "| Item       | Price ($) | Rating (1-5) | Delivery Time (min) |\n",
    "|------------|-----------|--------------|----------------------|\n",
    "| Pizza      | 12        | 4.5          | 30                   |\n",
    "| Burger     | 8         | 3.7          | 20                   |\n",
    "| Sushi      | 20        | 4.8          | 45                   |\n",
    "| Salad      | 15        | 3.2          | 25                   |\n",
    "```\n",
    "\n",
    "1. Compute Min and Max Values:\n",
    "   - \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\) for each feature (Price, Rating, Delivery Time).\n",
    "\n",
    "2. Apply Min-Max Scaling:\n",
    "   - Use the formula for each feature and scale the values.\n",
    "\n",
    "3. Scaled Dataset:\n",
    "   ```plaintext\n",
    "   | Item       | Price ($) | Rating (1-5) | Delivery Time (min) |\n",
    "   |------------|-----------|--------------|----------------------|\n",
    "   | Pizza      | 0.4       | 0.75         | 0.375                |\n",
    "   | Burger     | 0.0       | 0.25         | 0.0                  |\n",
    "   | Sushi      | 1.0       | 1.0          | 1.0                  |\n",
    "   | Salad      | 0.6       | 0.0          | 0.125                |\n",
    "   ```\n",
    "\n",
    "After Min-Max scaling, the values of each feature are now within the range [0, 1]. This ensures that all features contribute more equally to the recommendation system, preventing any particular feature from dominating the recommendation process due to differences in scale. The scaled dataset can then be used as input for building the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a334a-6a8e-4e8d-bf1b-2aef2afaf22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e24d554-9b54-405c-96cd-c32aa0a2dead",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5da9f-4478-4035-998a-a8aed0c1919a",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset containing numerous features, PCA (Principal Component Analysis) can be a valuable technique for reducing the dimensionality of the data. Reducing dimensionality is essential for several reasons, such as mitigating the curse of dimensionality, improving model performance, and enhancing interpretability. Here's a step-by-step guide on how you could use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Handle missing data: Address any missing values in the dataset through imputation or removal.\n",
    "   - Standardize the data: Ensure that all features are on the same scale by standardizing them (subtract the mean and divide by the standard deviation).\n",
    "\n",
    "2. **Apply PCA:**\n",
    "   - Calculate the covariance matrix: Compute the covariance matrix of the standardized features. The covariance matrix represents the relationships between different features.\n",
    "   - Compute eigenvectors and eigenvalues: Solve the eigenvalue decomposition problem for the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "   - Sort eigenvectors by eigenvalues: Arrange the eigenvectors in descending order based on their corresponding eigenvalues. The higher eigenvalues represent the directions of maximum variance.\n",
    "   - Choose the number of principal components: Decide on the number of principal components (eigenvectors) to retain. This can be based on the explained variance or a predefined number of components.\n",
    "\n",
    "3. **Project the Data:**\n",
    "   - Multiply the original standardized data by the selected top eigenvectors to obtain the reduced-dimensional representation.\n",
    "   - This results in a new set of uncorrelated features (principal components) that capture the most significant variance in the original data.\n",
    "\n",
    "4. **Model Training:**\n",
    "   - Use the reduced-dimensional dataset with principal components as input features for training your stock price prediction model.\n",
    "   - You can employ various regression models, neural networks, or other appropriate techniques depending on the nature of your prediction task.\n",
    "\n",
    "5. **Interpretation and Validation:**\n",
    "   - Analyze the importance of each principal component in terms of the variance it captures and its impact on the prediction task.\n",
    "   - Validate the model performance using appropriate metrics and ensure that dimensionality reduction does not lead to a significant loss of predictive accuracy.\n",
    "\n",
    "Here's a simplified example using a hypothetical dataset:\n",
    "\n",
    "```plaintext\n",
    "| Company  | Financial Feature 1 | Financial Feature 2 | Market Trend 1 | Market Trend 2 | Stock Price |\n",
    "|----------|----------------------|----------------------|-----------------|-----------------|-------------|\n",
    "| ABC      | 1000                 | 500                  | 1.2             | 0.8             | 150         |\n",
    "| XYZ      | 800                  | 300                  | 0.5             | 1.5             | 120         |\n",
    "| PQR      | 1200                 | 600                  | 0.9             | 1.2             | 180         |\n",
    "```\n",
    "\n",
    "After applying PCA, you would obtain a reduced-dimensional representation of the dataset with fewer features, such as principal component 1 (PC1) and principal component 2 (PC2), which could be used for model training:\n",
    "\n",
    "```plaintext\n",
    "| Company  | PC1                  | PC2                  | Stock Price |\n",
    "|----------|----------------------|----------------------|-------------|\n",
    "| ABC      | 0.2                  | -0.1                 | 150         |\n",
    "| XYZ      | -0.3                 | 0.2                  | 120         |\n",
    "| PQR      | 0.1                  | 0.3                  | 180         |\n",
    "```\n",
    "\n",
    "In this example, PC1 and PC2 represent linear combinations of the original features, capturing the most significant variance in the data. The reduced-dimensional dataset can now be used for training a stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277218b-b6f8-4e66-8c07-d5e632d709d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f933bdd8-d7ae-4c51-a757-437621aad2d7",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bcfa6-5a8a-444a-ace1-52bc8f453a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092c978-c594-47e3-ab39-d09fa1597572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94f0aa0f-e54e-49e2-92de-339422072dea",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d692531-3a4a-4c9f-9f71-e460d2cd27ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
